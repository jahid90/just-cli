package lexer

import (
	"fmt"

	"github.com/jahid90/just/lib"
)

// TokenBuffer A buffer that holds tokensgenerated by the lexer
type TokenBuffer struct {
	current int
	length  int
	tokens  []*Token
}

// AddToken Add a token to the stream
func (t *TokenBuffer) AddToken(token *Token) {
	t.tokens = append(t.tokens, token)
	t.length++
}

// AppendEOFToken Adds an EOF token to the end of the stream
func (t *TokenBuffer) appendEOFToken() {
	eofToken := &Token{Type: EOF, Position: -1, Value: "EOF"}

	t.AddToken(eofToken)
}

// Start Start of the stream
func (t *TokenBuffer) Start() int {
	return 0

}

// End End of the stream
func (t *TokenBuffer) End() int {
	return t.length
}

// ForwardToEnd Updates the current position of the stream to its end
func (t *TokenBuffer) ForwardToEnd() {
	t.current = t.length - 1
}

// Print print
func (t *TokenBuffer) Print() {
	fmt.Println("current: " + fmt.Sprint(t.current) + ", length: " + fmt.Sprint(t.length))
	for _, token := range t.tokens {
		fmt.Println(token)
	}
}

// HasNext Evaluates whether there are more tokens to be consumed
func (t *TokenBuffer) HasNext() bool {

	if t.current >= t.length {
		return false
	}

	if t.tokens[t.current].Type == EOF {
		return false
	}

	return true
}

// Next Returns the next token
// Panics if read beyond the available tokens; call HasNext() first
func (t *TokenBuffer) Next() *Token {

	token := t.Peek()
	t.current++

	return token
}

// NextN Returns the nth next token
func (t *TokenBuffer) NextN(n int) *Token {
	token := t.PeekN(n)
	t.current += n

	return token
}

// Peek Returns the next token without altering the stream state
func (t *TokenBuffer) Peek() *Token {
	if t.current >= t.length {
		panic("Peek: End of stream")
	}

	return t.tokens[t.current]
}

// PeekN Returns the nth next token without altering the stream state
func (t *TokenBuffer) PeekN(n int) *Token {

	idx := t.current + n

	if idx >= t.length {
		panic("PeekN: End of stream")
	}

	return t.tokens[idx]
}

// TakeBetween Returns all the tokens between start inclusive and end exclusive
// Appends an EOF Token if its not the last token type
func (t *TokenBuffer) TakeBetween(start int, end int) *TokenBuffer {

	if lib.DEBUG {
		fmt.Println("Taking [" + fmt.Sprint(start) + ", " + fmt.Sprint(end) + ")")
		fmt.Println("Current: " + fmt.Sprint(t.current) + ", Length: " + fmt.Sprint(t.length))
	}

	if start > end {
		panic("Take: start greater than end")
	}

	if start < 0 {
		panic("Take: start can not be less than 0")
	}

	if end > t.length {
		panic("Take: end can not be after stream's end")
	}

	if start == end {
		tb := NewTokenBuffer(nil)
		tb.appendEOFToken()

		return tb
	}

	tb := NewTokenBuffer(t.tokens[start:end])
	if tb.tokens[tb.length-1].Type != EOF {
		tb.appendEOFToken()
	}
	if lib.DEBUG {
		tb.Print()
	}

	return tb
}

// NewTokenBuffer Creates an instance of a token buffer
func NewTokenBuffer(tokens []*Token) *TokenBuffer {

	t := make([]*Token, len(tokens))
	copy(t, tokens)

	return &TokenBuffer{
		current: 0,
		length:  len(tokens),
		tokens:  t,
	}
}
